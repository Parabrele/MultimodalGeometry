{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b901501c",
   "metadata": {},
   "source": [
    "Feed modality wise centered embeddings *during training*, then at inference, add +1 to topk, and add both $mu_{\\mathrm{txt}}$ and $mu_{\\mathrm{img}}$ to the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd56f1",
   "metadata": {},
   "source": [
    "# Measure General SAE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01a026",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d45675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import metrics\n",
    "from src.utils import (\n",
    "    get_coco, get_laion, get_imagenet,\n",
    "    _train,\n",
    "    measure_everything, measure_bridge, get_bridge_norms,\n",
    ")\n",
    "import importlib\n",
    "import src.utils as utils\n",
    "importlib.reload(utils)\n",
    "from src.utils import (\n",
    "    plot_modality_energy,\n",
    "    plot_cosim_histogram, plot_pca, fit_and_plot_pca, _intervene,\n",
    "    generate_visualization, linear_separability, data_classifiability\n",
    ")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a588a0",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45925de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = \"siglip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41842058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_model == \"clip\":\n",
    "    model_name = \"openai/clip-vit-base-patch32\"\n",
    "    d_model = 512\n",
    "    beta = 8e-4\n",
    "elif use_model == \"clip-L\":\n",
    "    model_name = \"openai/clip-vit-large-patch14\"\n",
    "    d_model = 768\n",
    "    beta = 1e-4\n",
    "elif use_model == \"openclip\":\n",
    "    model_name=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n",
    "    d_model = 512\n",
    "    beta = 4e-4\n",
    "elif use_model == \"openclip-L\":\n",
    "    model_name=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"\n",
    "    d_model = 768\n",
    "    beta = 2e-4\n",
    "elif use_model == \"siglip\":\n",
    "    model_name = \"google/siglip-base-patch16-224\"\n",
    "    d_model = 768\n",
    "    beta = 4e-4\n",
    "elif use_model == \"siglip2\":\n",
    "    model_name = \"google/siglip2-base-patch16-224\"\n",
    "    d_model = 768\n",
    "    beta = 7e-5\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model {use_model} not recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "laion_loader = get_laion(device=device, model_name=model_name, batch_size=512)\n",
    "# coco_loader = get_coco(device=device)\n",
    "# imagenet_test_loader, class_embeddings = get_imagenet(batch_size=512, model_name=model_name, device=device)\n",
    "# imagenet_train_loader, class_embeddings = get_imagenet(batch_size=512, model_name=model_name, device=device, split=\"train\")\n",
    "\n",
    "train_loader = laion_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05740474",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAE, SSAE_name, _ = _train(beta=0, train_loader=train_loader, model_name=model_name, d_model=d_model, expansion_factor=8, UseTOPK=False, device=device, mean=True)#, force_retrain=True, save_quand_meme=True) # Standard SAE\n",
    "ASAE, ASAE_name, _ = _train(beta=beta, train_loader=train_loader, model_name=model_name, d_model=d_model, expansion_factor=8, UseTOPK=False, device=device, mean=True)#, force_retrain=True, save_quand_meme=True) # Aligned SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aabc00",
   "metadata": {},
   "source": [
    "## Energy & Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc212373",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAE.metrics = measure_everything(SSAE, train_loader, device, return_sqr=True)\n",
    "ASAE.metrics = measure_everything(ASAE, train_loader, device, return_sqr=True)\n",
    "print(f\"Energy per concept: {ASAE.metrics.E[torch.argsort(ASAE.metrics.E, descending=True)[:10]]}\")\n",
    "print(f\"Frequency per concept: {ASAE.metrics.f[torch.argsort(ASAE.metrics.E, descending=True)[:10]]}\")\n",
    "print(f\"Modality score per concept: {ASAE.metrics.mu[torch.argsort(ASAE.metrics.E, descending=True)[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAE_bridge = {\n",
    "    \"bridge_sigma\" : measure_bridge(SSAE, train_loader, SSAE.metrics.E, SSAE.metrics.E_Img, SSAE.metrics.E_Txt, device, center=False, normalize=False, null_C=False, null_D=False).cpu().detach(),\n",
    "    \"bridge_gamma\" : measure_bridge(SSAE, train_loader, SSAE.metrics.E, SSAE.metrics.E_Img, SSAE.metrics.E_Txt, device, center=False, normalize=False, null_C=False, null_D=False, weight_type=\"OT\").cpu().detach(),\n",
    "}\n",
    "ASAE_bridge = {\n",
    "    \"bridge_sigma\" : measure_bridge(ASAE, train_loader, ASAE.metrics.E, ASAE.metrics.E_Img, ASAE.metrics.E_Txt, device, center=False, normalize=False, null_C=False, null_D=False).cpu().detach(),\n",
    "    \"bridge_gamma\" : measure_bridge(ASAE, train_loader, ASAE.metrics.E, ASAE.metrics.E_Img, ASAE.metrics.E_Txt, device, center=False, normalize=False, null_C=False, null_D=False, weight_type=\"OT\").cpu().detach(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cff2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bridge_norms(SSAE_bridge[\"bridge_sigma\"], SSAE)\n",
    "get_bridge_norms(ASAE_bridge[\"bridge_sigma\"], ASAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba685d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SSAE.metrics)\n",
    "print(ASAE.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3906b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_modality_energy(SSAE, SSAE_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_modality_energy(ASAE, ASAE_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fee605",
   "metadata": {},
   "source": [
    "## Geometric Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8b7c2",
   "metadata": {},
   "source": [
    "#### 0. Unimodal \"Biases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ASAE.dictionary._fused_dictionary.cpu().detach().numpy()\n",
    "mu = ASAE.metrics.mu.cpu().numpy()\n",
    "f = ASAE.metrics.f\n",
    "E = ASAE.metrics.E.cpu().numpy()\n",
    "eps = 1e-3\n",
    "mask = (mu <= eps) & (E > 1e-3)\n",
    "print(ASAE.metrics.E.sum())\n",
    "print(ASAE.metrics.E[mask].sum()/ASAE.metrics.E.sum())\n",
    "print(mask.sum())\n",
    "D = D[mask]\n",
    "f = f[mask]\n",
    "print(D.shape)\n",
    "print(f\"Total Frequency : {f.sum().item()}\")\n",
    "DDT = D @ D.T\n",
    "print(DDT.mean())\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(DDT, vmin=0.5, vmax=1, annot=True, fmt=\".2f\", cmap='viridis', cbar=True,\n",
    "            annot_kws={\"size\": 8, \"color\": \"black\", \"ha\": \"center\", \"va\": \"center\"})\n",
    "plt.show()\n",
    "\n",
    "D = ASAE.dictionary._fused_dictionary.cpu().detach().numpy()\n",
    "mu = ASAE.metrics.mu.cpu().numpy()\n",
    "f = ASAE.metrics.f\n",
    "E = ASAE.metrics.E.cpu().numpy()\n",
    "eps = 1e-3\n",
    "mask = (mu >= (1 - eps)) & (E > 1e-3)\n",
    "print(ASAE.metrics.E.sum())\n",
    "print(ASAE.metrics.E[mask].sum()/ASAE.metrics.E.sum())\n",
    "print(mask.sum())\n",
    "D = D[mask]\n",
    "f = f[mask]\n",
    "print(D.shape)\n",
    "print(f\"Total Frequency : {f.sum().item()}\")\n",
    "DDT = D @ D.T\n",
    "print(DDT.mean())\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(DDT, vmin=0.5, vmax=1, annot=True, fmt=\".2f\", cmap='viridis', cbar=True,\n",
    "            annot_kws={\"size\": 8, \"color\": \"black\", \"ha\": \"center\", \"va\": \"center\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594ab03",
   "metadata": {},
   "source": [
    "#### 1. Cosim Histograms & Data Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = train_loader.dataset.tensors[0].cpu()\n",
    "T = train_loader.dataset.tensors[1].cpu()\n",
    "D_ict = torch.cat([I.cpu(), T.cpu()], dim=0)\n",
    "I_mu = I.mean(dim=0)\n",
    "T_mu = T.mean(dim=0)\n",
    "D_mu = (I_mu + T_mu) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cosim_histogram(I, T, SSAE_name)\n",
    "plot_cosim_histogram(I, T, ASAE_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \"PCA\"s of D, but not on the top 2 PCs, instead on (1) D_mu, (2) I_mu - T_mu\n",
    "\n",
    "d1 = D_mu.cpu().numpy()\n",
    "d1 = d1 / np.linalg.norm(d1)\n",
    "d2 = I_mu.cpu().numpy() - T_mu.cpu().numpy()\n",
    "d2 = d2 / np.linalg.norm(d2)\n",
    "\n",
    "plot_pca(D_ict, d1, d2, I_mu, T_mu, D_mu, title='Projection of D on d1 = D_mu and d2 = I_mu - T_mu', save_title='data_pca_d1_d2', sae_name=SSAE_name)\n",
    "plot_pca(D_ict, d1, d2, I_mu, T_mu, D_mu, title='Projection of D on d1 = D_mu and d2 = I_mu - T_mu', save_title='data_pca_d1_d2', sae_name=ASAE_name)\n",
    "\n",
    "fit_and_plot_pca(I, T, D_ict, SSAE_name)\n",
    "fit_and_plot_pca(I, T, D_ict, ASAE_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c446fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_bi_S, T_bi_S, _ = _intervene(SSAE, train_loader, SSAE.metrics.mu, device, return_D=False)\n",
    "# plot_cosim_histogram(I_bi_S, T_bi_S, B_bi_S, SSAE_name, subtitle=\"Unimodal concepts removed\")\n",
    "I_bi_A, T_bi_A, _ = _intervene(ASAE, train_loader, ASAE.metrics.mu, device, return_D=False)\n",
    "# plot_cosim_histogram(I_bi_A, T_bi_A, B_bi_A, ASAE_name, subtitle=\"Unimodal concepts removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_bi_S = torch.cat([I_bi_S.cpu(), T_bi_S.cpu()], dim=0).cpu()\n",
    "B_bi_A = torch.cat([I_bi_A.cpu(), T_bi_A.cpu()], dim=0).cpu()\n",
    "fit_and_plot_pca(I_bi_S, T_bi_S, B_bi_S, SSAE_name, save_title='data_proj_pca_pc1_pc2')\n",
    "fit_and_plot_pca(I_bi_A, T_bi_A, B_bi_A, ASAE_name, save_title='data_proj_pca_pc1_pc2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c03cba",
   "metadata": {},
   "source": [
    "#### 3. Dictionary Atoms Linear Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02908900",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_visualization(SSAE, SSAE_name, SSAE.metrics.mu, SSAE.metrics.E, SSAE.metrics.f, SSAE_bridge[\"bridge_sigma\"])\n",
    "generate_visualization(ASAE, ASAE_name, ASAE.metrics.mu, ASAE.metrics.E, ASAE.metrics.f, ASAE_bridge[\"bridge_sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73a82b",
   "metadata": {},
   "source": [
    "##### 3.1. Features classifying features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_separability(SSAE, SSAE_name, eps=modality_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_separability(ASAE, ASAE_name, eps=modality_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a482b39",
   "metadata": {},
   "source": [
    "##### 3.2. Features classifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57339331",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifiability(SSAE, I, T, SSAE_name, eps=modality_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifiability(ASAE, I, T, ASAE_name, eps=modality_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90372ff",
   "metadata": {},
   "source": [
    "##### 3.3. Dictionary orthogonality : $D_I \\bot D_T \\bot D_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_orthogonality(sae, modality_score):\n",
    "    D = sae.dictionary._fused_dictionary.cpu().detach().numpy()\n",
    "    D = D / np.linalg.norm(D, axis=1, keepdims=True)  # Normalize each vector to unit length\n",
    "    orthogonality = (np.dot(D, D.T))\n",
    "    I_mask = (modality_score > 0.9).cpu().numpy()\n",
    "    T_mask = (modality_score < 0.1).cpu().numpy()\n",
    "    B_mask = (modality_score <= 0.9).cpu().numpy() & (modality_score >= 0.1).cpu().numpy()\n",
    "    sae.metrics._add_more({\n",
    "        'orthogonality': {\n",
    "            'I_ortho_T': orthogonality[I_mask][:, T_mask].mean(),\n",
    "            'I_ortho_B': orthogonality[I_mask][:, B_mask].mean(),\n",
    "            'T_ortho_B': orthogonality[T_mask][:, B_mask].mean(),\n",
    "            'I_ortho_I': orthogonality[I_mask][:, I_mask].mean(),\n",
    "            'T_ortho_T': orthogonality[T_mask][:, T_mask].mean(),\n",
    "            'B_ortho_B': orthogonality[B_mask][:, B_mask].mean(),\n",
    "        }\n",
    "    }, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0059b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dict_orthogonality(SSAE, SSAE.metrics.mu)\n",
    "get_dict_orthogonality(ASAE, ASAE.metrics.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de3ad3",
   "metadata": {},
   "source": [
    "#### 4. Modality Gap & Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_modality_gap(I, T, sae, data_name=\"default\"):\n",
    "    # Measure the modality gap :\n",
    "    # - norm of difference in mean image and text embeddings\n",
    "    # - wasserstein distance between the image and text embeddings distributions.\n",
    "    N_subsample = int(1e4) # dataset has 1e6 images and texts, subsample to 1e4 for speed.\n",
    "    # raise\n",
    "    # # TODO : try modality gap wasserstein without \n",
    "    p1 = torch.randperm(I.shape[0])[:N_subsample]\n",
    "    p2 = torch.randperm(T.shape[0])[:N_subsample]\n",
    "    T = T[p2].cpu()\n",
    "    I = I[p1].cpu() # shape (N_subsample, D)\n",
    "    I_mean = I.mean(axis=0) # shape (D,)\n",
    "    T_mean = T.mean(axis=0)\n",
    "    DiM = np.linalg.norm(I_mean - T_mean)\n",
    "    c = metrics.Wasserstein(I, T, metric='cosim')\n",
    "    print(f\"Modality Gap: DiM: {DiM}, OT cost: {c}\")\n",
    "    sae.metrics._add_more({\n",
    "        f\"modality_gap_{data_name}\": {\n",
    "            \"DiM\": DiM,\n",
    "            \"Wasserstein\": c,\n",
    "        }\n",
    "    }, save=True)\n",
    "\n",
    "def get_contrastive_loss(I, T, sae, data_name=\"default\"):\n",
    "    # Track Contrastive Loss & accuracy for Image-Text pairs\n",
    "    # TODO : retrain the logit_scale parameter to fit the data\n",
    "    batch_size = 256\n",
    "    num_batches = I.shape[0] // batch_size\n",
    "\n",
    "    class LogitScale(torch.nn.Module):\n",
    "        def __init__(self, initial_value=4.6052):\n",
    "            super(LogitScale, self).__init__()\n",
    "            self.logit_scale = torch.nn.Parameter(torch.tensor(initial_value))\n",
    "\n",
    "        def forward(self, x):\n",
    "            return torch.exp(self.logit_scale) * x\n",
    "    logit_scale = LogitScale()\n",
    "    optimizer = torch.optim.Adam(logit_scale.parameters(), lr=1e-3)\n",
    "\n",
    "    for ep in range(1, 3):\n",
    "        contrastive_loss = 0.0\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "        p = 0\n",
    "        for i in tqdm(range(num_batches)):\n",
    "            I_batch = I[i * batch_size:(i + 1) * batch_size]\n",
    "            T_batch = T[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            I_batch = I_batch / (I_batch.norm(dim=1, keepdim=True) + 1e-8)\n",
    "            T_batch = T_batch / (T_batch.norm(dim=1, keepdim=True) + 1e-8)\n",
    "            logits_per_image = I_batch @ T_batch.T\n",
    "            logits_per_text = T_batch @ I_batch.T\n",
    "\n",
    "            # logit_scale = torch.exp(torch.tensor(4.6052)) # from CLIP ViT-B/32\n",
    "\n",
    "            logits_per_image = logit_scale(logits_per_image)\n",
    "            logits_per_text = logit_scale(logits_per_text)\n",
    "\n",
    "            labels = torch.arange(batch_size, device=I_batch.device)\n",
    "            loss_image = F.cross_entropy(logits_per_image, labels)\n",
    "            loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            contrastive_loss += loss.item()\n",
    "\n",
    "            acc_image = (logits_per_image.argmax(dim=1) == labels).sum().item()\n",
    "            acc_text = (logits_per_text.argmax(dim=1) == labels).sum().item()\n",
    "            correct_pairs += (acc_image + acc_text) / 2\n",
    "            total_pairs += batch_size\n",
    "\n",
    "            # p : probability of correct labels\n",
    "            p_image = torch.softmax(logits_per_image, dim=1)\n",
    "            p_image = p_image[torch.arange(batch_size, device=I_batch.device), labels].mean().item()\n",
    "            p_text = torch.softmax(logits_per_text, dim=1)\n",
    "            p_text = p_text[torch.arange(batch_size, device=I_batch.device), labels].mean().item()\n",
    "            p += (p_image + p_text) / 2\n",
    "        contrastive_loss /= num_batches\n",
    "        accuracy = correct_pairs / total_pairs\n",
    "        print(f\"Contrastive Loss: {contrastive_loss:.4f}, Accuracy: {accuracy:.4f}, Probability: {p / num_batches:.4f}\")\n",
    "    sae.metrics._add_more({\n",
    "        f\"contrastive_loss_{data_name}\": {\n",
    "            \"loss\": contrastive_loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"probability\": p / num_batches,\n",
    "        }\n",
    "    }, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf59263",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_modality_gap(train_loader.dataset.tensors[0], train_loader.dataset.tensors[1], SSAE)\n",
    "get_modality_gap(train_loader.dataset.tensors[0], train_loader.dataset.tensors[1], ASAE)\n",
    "\n",
    "get_contrastive_loss(train_loader.dataset.tensors[0], train_loader.dataset.tensors[1], SSAE)\n",
    "get_contrastive_loss(train_loader.dataset.tensors[0], train_loader.dataset.tensors[1], ASAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_modality_gap(I_bi_S, T_bi_S, SSAE, data_name=\"bimodal\")\n",
    "get_modality_gap(I_bi_A, T_bi_A, ASAE, data_name=\"bimodal\")\n",
    "\n",
    "get_contrastive_loss(I_bi_S, T_bi_S, SSAE, data_name=\"bimodal\")\n",
    "get_contrastive_loss(I_bi_A, T_bi_A, ASAE, data_name=\"bimodal\")\n",
    "\n",
    "del I_bi_S, T_bi_S, I_bi_A, T_bi_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_ae_S, T_ae_S, _ = _intervene(SSAE, train_loader, SSAE.metrics.mu, device, eps=0.0, return_D=False)\n",
    "I_ae_A, T_ae_A, _ = _intervene(ASAE, train_loader, ASAE.metrics.mu, device, eps=0.0, return_D=False)\n",
    "\n",
    "# plot_cosim_histogram(I_ae_S, T_ae_S, B_ae_S, SSAE_name, subtitle=\"AE\")\n",
    "# plot_cosim_histogram(I_ae_A, T_ae_A, B_ae_A, ASAE_name, subtitle=\"AE\")\n",
    "# fit_and_plot_pca(I_ae_S, T_ae_S, B_ae_S, SSAE_name, save_title='data_ae_pca_pc1_pc2')\n",
    "# fit_and_plot_pca(I_ae_A, T_ae_A, B_ae_A, ASAE_name, save_title='data_ae_pca_pc1_pc2')\n",
    "get_modality_gap(I_ae_S, T_ae_S, SSAE, data_name=\"AE\")\n",
    "get_modality_gap(I_ae_A, T_ae_A, ASAE, data_name=\"AE\")\n",
    "get_contrastive_loss(I_ae_S, T_ae_S, SSAE, data_name=\"AE\")\n",
    "get_contrastive_loss(I_ae_A, T_ae_A, ASAE, data_name=\"AE\")\n",
    "\n",
    "del I_ae_S, T_ae_S, I_ae_A, T_ae_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_uni_S, T_uni_S, _ = _intervene(SSAE, train_loader, SSAE.metrics.mu, device, complement=True, return_D=True)\n",
    "I_uni_A, T_uni_A, _ = _intervene(ASAE, train_loader, ASAE.metrics.mu, device, complement=True, return_D=True)\n",
    "get_modality_gap(I_uni_S, T_uni_S, SSAE, data_name=\"unimodal\")\n",
    "get_modality_gap(I_uni_A, T_uni_A, ASAE, data_name=\"unimodal\")\n",
    "\n",
    "get_contrastive_loss(I_uni_S, T_uni_S, SSAE, data_name=\"unimodal\")\n",
    "get_contrastive_loss(I_uni_A, T_uni_A, ASAE, data_name=\"unimodal\")\n",
    "\n",
    "del I_uni_S, T_uni_S, I_uni_A, T_uni_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SSAE.metrics)\n",
    "print(ASAE.metrics)\n",
    "\n",
    "# torch.save(SSAE.metrics, f\"./figures/{SSAE_name.replace('.pt', '')}/metrics.pt\")\n",
    "# torch.save(ASAE.metrics, f\"./figures/{ASAE_name.replace('.pt', '')}/metrics.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef5366",
   "metadata": {},
   "source": [
    "## Modality Gap\n",
    "### CLIP\n",
    "- Default : 0.72\n",
    "- SAE\n",
    "    - AE : 0.72\n",
    "    - Bi : 0.04\n",
    "    - Uni : 0.72\n",
    "- SAE-A\n",
    "    - AE : 0.72\n",
    "    - Bi : 0.04\n",
    "    - Uni : 0.72\n",
    "\n",
    "### OpenCLIP\n",
    "- Default : 0.51\n",
    "- SAE\n",
    "    - AE : 0.51\n",
    "    - Bi : 0.07\n",
    "    - Uni : 0.53\n",
    "- SAE-A\n",
    "    - AE : 0.51\n",
    "    - Bi : 0.06\n",
    "    - Uni : 0.49\n",
    "\n",
    "### SIGLIP\n",
    "- Default : 0.97\n",
    "- SAE\n",
    "    - AE : 0.97\n",
    "    - Bi : 0.032\n",
    "    - Uni : 1.08\n",
    "- SAE-A\n",
    "    - AE : 0.97\n",
    "    - Bi : 0.026\n",
    "    - Uni : 1.09\n",
    "\n",
    "### SIGLIP2\n",
    "- Default : 1.08\n",
    "- SAE\n",
    "    - AE : 1.08\n",
    "    - Bi : 0.04\n",
    "    - Uni : 1.09\n",
    "- SAE-A\n",
    "    - AE : 1.08\n",
    "    - Bi : 0.05\n",
    "    - Uni : 1.09\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd359b",
   "metadata": {},
   "source": [
    "## Recall\n",
    "### CLIP\n",
    "- Default : 0.96\n",
    "- SAE\n",
    "    - AE : 0.92\n",
    "    - Bi : 0.70\n",
    "    - Uni : 0.17\n",
    "- SAE-A\n",
    "    - AE : 0.88\n",
    "    - Bi : 0.76\n",
    "    - Uni : 0.006\n",
    "\n",
    "### OpenCLIP\n",
    "- Default : 0.98\n",
    "- SAE\n",
    "    - AE : 0.95\n",
    "    - Bi : 0.91\n",
    "    - Uni : 0.02\n",
    "- SAE-A\n",
    "    - AE : 0.95\n",
    "    - Bi : 0.93\n",
    "    - Uni : 0.007\n",
    "\n",
    "### SIGLIP\n",
    "- Default : 0.96\n",
    "- SAE\n",
    "    - AE : 0.91\n",
    "    - Bi : 0.88\n",
    "    - Uni : 0.02\n",
    "- SAE-A\n",
    "    - AE : 0.91\n",
    "    - Bi : 0.9010\n",
    "    - Uni : 0.007\n",
    "\n",
    "### SIGLIP2\n",
    "- Default : 0.36\n",
    "- SAE\n",
    "    - AE : 0.30\n",
    "    - Bi : 0.29\n",
    "    - Uni : 0.02\n",
    "- SAE-A\n",
    "    - AE : 0.30\n",
    "    - Bi : 0.31\n",
    "    - Uni : 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
